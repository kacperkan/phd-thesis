% --- repeat the title (AT: haven't found a more elegant way to do this...)
{
\vspace{2.0em}
\centering
\Large
\textbf{\blendfields: Few-Shot Example-Driven Facial Modeling} \\
\vspace{0.5em}
 Supplementary Material \\ \vspace{1.0em} }

 \section{Potential social impact} Our motivation for this work was to enable
 the creation of 3D avatars that could be used as communication devices in the
 remote working era.
  As our approach stems from blendshapes~\cite{lewis2014practice}, these
  avatars are easily adjustable via texture coloring and may be used for
  entertainment.
  We note, however, that the potential misuse of our work includes using it as
  deep fakes.
  We highly discourage such usage.
  One of our future directions includes detecting fake images generated by our
  method.
  At the same time, we highlight the importance of \blendfields---in the
  presence of closed technologies~\cite{ma2021pixel,cao2022authentic}, it is
  crucial to democratize techniques for personalized avatar creation.
  We achieve that by limiting the required data volume to train a single
  model.
  As history shows, when given an open, readily available technology for
  generative modeling of images~\cite{rombach2022high}, users can scrutinize
  it with unprecedented thoroughness, thus raising the general awareness of
  potential misuses.

\section{Concurrent Works}
  Gao~\etal~\cite{gao2022reconstructing} and Xu~\etal~\cite{xu2023avatarmav}
  also use an interpolation between known expressions to combine multiple
  neural radiance fields trained for those expressions.
  However, their approach interpolates between grids of latent
  vectors~\cite{mueller2022instant} globally.
  The interpolation weights are taken from blendshape coefficients.

  Zielonka~\etal~\cite{zielonka2022instant} use a parametric head model to
  canonicalize 3D points similarly to our ends.
  However, instead of building a tetrahedral cage around the head, they
  smoothly assign each face triangle to 3D points.
  Then they canonicalize points using transformations that each of the
  assigned triangles undergoes for a given expression.
  They concatenate 3D points with the expression code from
  FLAME~\cite{li2017flame} to model expression-dependent effects.

  \input{tabs/\blendfieldsdirname/ablation-expressions-sup}
\section{Additional results}
  \subsection{Ablating number of expressions}
    We ablate over the number of used expressions during the training.
    To evaluate the effect of the number of expressions, we add consecutive
    frames to the training set (starting from a single, neutral one), \ie, the
    training set has $\iExpr{<}\nExpr$ expressions.
    We train \blendfields for such a set for each subject separately.
    We then average the results for a given $\iExpr$ across subjects.
    We present the results
    in~\cref{tab:blendfields-ablation-num-expressions-fix}.
    When selecting the training expressions, we aim to choose those that show
    all wrinkles when combined.
    We can see from~\cref{fig:blendfields-qualitative-ablation-expressions}
    that if removed, \eg, the expressions with eyebrows raised, then the model
    cannot render wrinkles on the forehead.
    In summary, increasing the number of expressions improves the quality
    results with diminishing returns when $\nExpr{>}5$, while $\nExpr{=}5$
    provides a sufficient trade-off between the data capture cost and the
    quality.
    % Moreover, we posit that $\nExpr{=}5$ is enough to capture most facial
    % details and get consistent, high-quality results.

    \input{fig/\blendfieldsdirname/supplementary-training-frames}
  \subsection{Training frames}
    We present in~\cref{fig:blendfields-supplementary-training-frames} example
    training frames for one of the subjects.
    Each frame is a multi-view frame captured with ${\approx} 35$ cameras (the
    number of available cameras varied slightly between subjects).

    \input{tabs/\blendfieldsdirname/quantitative-results-without-masking}
  \subsection{Quantitative results with background}
    We compare \blendfields and the baselines similarly
    to~\cref{subsec:blendfields-realistic-human-captures}.
    However, in this experiment, we deliberately include the background in
    metric calculation.
    We show the results in
    \cref{tab:blendfields-quantitative-results-without-masking}.
    In all the cases, \blendfields performs best even though the method was
    not designed to model the background accurately.
    Additionally, as HyperNeRF~\cite{park2021hypernerf},
    NeRFies~\cite{park2021nerfies}, and NeRF~\cite{mildenhall2020nerf} do not
    have any mechanism to disambiguate between the foreground and the
    background, the metrics are significantly worse when including the latter.

  \subsection{Additional qualitative results}

    We show in~\cref{fig:blendfields-qualitative-other-baselines} results of
    baselines that do not rely on parametric models of the
    face~\cite{li2017flame}.
    Compared to \blendfields, they cannot render high-fidelity faces.
    The issue comes from the assumed data sparsity---those approaches rely on
    the interpolation in the training data.
    As we assume access to just a few frames, there is no continuity in the
    training data that would guide them to interpolate between known
    expressions.
    \blendfields presents superior results given novel expressions even with such a sparse dataset.
    results.

    \clearpage
    \input{fig/\blendfieldsdirname/qualitative-ablation-supplementary}
    \input{fig/\blendfieldsdirname/qualitative-other-baselines}